{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "Answer: Boosting is a technique in machine learning where many weak models are combined to create one strong model. A weak model is something that performs just a little better than random guessing, like a simple decision tree. In boosting, models are not trained all at once; instead, they are trained one after another. After the first model is trained, the mistakes it makes are identified. The data points that were wrongly predicted are given more importance, so the next model focuses more on them. This process continues for several rounds, and each new model tries to correct the errors made by the earlier models. At the end, all the weak models are combined, usually by giving more weight to better models, to make the final prediction. This way, boosting improves weak learners by continuously correcting errors and focusing on difficult examples, resulting in a strong and accurate model.\n",
        "\n",
        "How boosting improves weak learners (points):\n",
        "\n",
        "1) Each new learner focuses on correcting mistakes made by the previous learner.\n",
        "\n",
        "2) Wrongly predicted examples are given more weight, so future models pay more attention to them.\n",
        "\n",
        "3) Errors keep reducing in each iteration as the model learns from past mistakes.\n",
        "\n",
        "4) Multiple weak models are combined to create one strong model.\n",
        "\n",
        "5) Boosting adapts to difficult data points, improving overall performance.\n",
        "\n",
        "\n",
        "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "Answer: **Answer:**\n",
        "\n",
        "The main difference between **AdaBoost** and **Gradient Boosting** lies in how they train each new model.\n",
        "\n",
        "In **AdaBoost**, models are trained sequentially, and each new model focuses on the data points that were misclassified by the previous model. It does this by **increasing the weights of misclassified samples**, so the next model pays more attention to them. The final prediction is made by combining all models using **weighted voting**, where more accurate models have higher influence.\n",
        "\n",
        "In **Gradient Boosting**, models are also trained sequentially, but each new model is trained to correct the **residual errors (differences between actual and predicted values)** of the previous model. Instead of adjusting sample weights, Gradient Boosting uses **gradient descent** to minimize a loss function. Each new model learns the gradient of the loss and improves the overall prediction by reducing this error step-by-step. The final model is obtained by **adding the predictions of all models**.\n",
        "\n",
        "So, while both are boosting methods, **AdaBoost changes sample weights to focus on misclassified data**, whereas **Gradient Boosting fits models to the residual errors using gradient descent**.\n",
        "\n",
        "Question 3: How does regularization help in XGBoost?\n",
        "\n",
        "Answer: **Answer (20 marks style):**\n",
        "\n",
        "Regularization in XGBoost plays a crucial role in improving the model’s performance by preventing overfitting and ensuring better generalization. XGBoost is an advanced boosting algorithm that builds decision trees sequentially. While boosting increases accuracy, it also increases the risk of overfitting, especially when trees become too complex or deep. Regularization addresses this issue by adding penalty terms to the objective function, which discourage overly complex models. XGBoost uses both **L1 (Lasso)** and **L2 (Ridge)** regularization. L1 regularization helps in feature selection by forcing some feature weights to become zero, while L2 regularization reduces the magnitude of leaf weights and stabilizes the model. This makes the model less sensitive to noise and prevents it from fitting random fluctuations in the training data. Additionally, XGBoost uses other parameters like **max_depth**, **min_child_weight**, and **gamma**, which also act as regularizers by limiting tree complexity. As a result, regularization in XGBoost helps maintain a balance between fitting the training data well and keeping the model simple, leading to improved accuracy and robustness on unseen data.\n",
        "\n",
        "\n",
        "Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "Answer: CatBoost is considered efficient for handling categorical data because it is specifically designed to process categories directly without converting them into many separate numeric columns. Most machine learning models require categorical features to be converted using techniques like one-hot encoding, which can increase data size and make the model slow. CatBoost avoids this by using special encoding techniques that transform categorical values into meaningful numbers. It uses target-based statistics (like average target value for each category) but in a safe way to avoid data leakage. This is done using ordered boosting, where the encoding for each row is calculated only from previous rows, not from future data. This helps CatBoost to learn patterns efficiently, reduce bias, and prevent overfitting. Because of this, CatBoost works faster and gives better accuracy when dealing with datasets having many categorical features.\n",
        "\n",
        "Why CatBoost is efficient for categorical data (points):\n",
        "\n",
        "1) It processes categorical features directly without one-hot encoding.\n",
        "\n",
        "2) It uses target-based encoding which reduces the need for manual preprocessing.\n",
        "\n",
        "3) Ordered boosting prevents data leakage and reduces overfitting.\n",
        "\n",
        "4) It handles high-cardinality categories efficiently.\n",
        "\n",
        "5) It results in faster training and better accuracy on categorical datasets.\n",
        "\n",
        "Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "Answer: Boosting techniques are preferred over bagging methods in real-world applications where high accuracy and better handling of difficult cases are required. Boosting works by training models sequentially and focusing on correcting the errors of previous models, which helps in reducing bias and improving prediction performance. Therefore, boosting is commonly used in applications such as:\n",
        "\n",
        "1) Fraud Detection: Detecting fraudulent transactions is difficult because fraud cases are rare and hard to identify. Boosting helps by focusing on these rare cases and improving detection accuracy.\n",
        "\n",
        "2) Credit Scoring and Risk Assessment: In banking and finance, accurate prediction of loan default or credit risk is crucial. Boosting provides better accuracy and reliability compared to bagging.\n",
        "\n",
        "3) Medical Diagnosis: For disease prediction and diagnosis, small errors can have serious consequences. Boosting improves precision by focusing on hard-to-classify cases.\n",
        "\n",
        "4) Customer Churn Prediction: Identifying customers who may leave a service requires high accuracy. Boosting helps in correctly predicting churn by learning from misclassified examples.\n",
        "\n",
        "5) Search Ranking and Recommendation Systems: These systems require highly accurate ranking and recommendations. Boosting improves performance by optimizing predictions based on previous errors.\n",
        "\n",
        "In these scenarios, boosting is preferred because it produces a stronger model by sequentially learning from mistakes, which often leads to better accuracy than bagging.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Datasets**\n",
        "● Use sklearn.datasets.load_breast_cancer() for classification tasks.\n",
        "\n",
        "● Use sklearn.datasets.fetch_california_housing() for regression\n",
        "tasks.\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "● Print the model accuracy\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "JRmVy7JPpw1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create AdaBoost Classifier\n",
        "model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"AdaBoost Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqYEMaHFLaX7",
        "outputId": "3ea6d20d-1f15-480a-d8bb-9f0c24b0d26d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "# Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "# Evaluate performance using R-squared score\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create Gradient Boosting Regressor\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the R-squared score\n",
        "print(\"Gradient Boosting Regressor R-squared Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MivAXwpqLl-S",
        "outputId": "2a9c4c98-db3b-4b27-fc05-714f70e2da1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared Score: 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "# Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "# Tune the learning rate using GridSearchCV\n",
        "# Print the best parameters and accuracy\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create XGBoost Classifier (without use_label_encoder)\n",
        "xgb = XGBClassifier(eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define parameter grid for learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# GridSearchCV to tune learning rate\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Predict using best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UJwYtdKL3cM",
        "outputId": "fa976a19-e8c0-428e-bacc-5c8adb7d3a8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "# Train a CatBoost Classifier\n",
        "# Plot the confusion matrix using seaborn\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    verbose=0\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot using seaborn\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "YwWNQh3yMuBh",
        "outputId": "65ad7d84-6cfc-443f-f430-2e42dc182aaf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior. The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "\n",
        "● Hyperparameter tuning strategy\n",
        "\n",
        "● Evaluation metrics you'd choose and why\n",
        "\n",
        "● How the business would benefit from your model\n",
        "\n",
        "\n",
        "Answer:\n",
        "1) Data Preprocessing\n",
        "\n",
        "    i) Convert some numeric columns to categorical.\n",
        "\n",
        "    ii) Introduce imbalance and missing values.\n",
        "\n",
        "    iii) Use CatBoost (handles categorical & missing values).\n",
        "\n",
        "2) Model Choice: CatBoost is best for categorical + missing data.\n",
        "\n",
        "3) Hyperparameter Tuning: Use RandomizedSearchCV for faster tuning.\n",
        "\n",
        "4) Evaluation Metrics: ROC-AUC, Precision, Recall, F1-score\n",
        "(Important for imbalanced data)\n",
        "\n",
        "5) Business Benefit\n",
        "\n",
        "    i) Better detection of risky customers\n",
        "\n",
        "    ii) Lower financial losses\n",
        "\n",
        "    iii) Better loan approval decisions"
      ],
      "metadata": {
        "id": "GY6G6r0QNOjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Load Breast Cancer dataset (as a substitute for loan default dataset)\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# ---------------------------\n",
        "# Simulate missing values\n",
        "# ---------------------------\n",
        "X_missing = X.copy()\n",
        "for col in X_missing.columns[:5]:\n",
        "    X_missing.loc[X_missing.sample(frac=0.1).index, col] = np.nan\n",
        "\n",
        "# ---------------------------\n",
        "# Simulate categorical columns\n",
        "# ---------------------------\n",
        "X_cat = X_missing.copy()\n",
        "X_cat['mean_radius_cat'] = pd.qcut(X_cat['mean radius'], 4, labels=['A','B','C','D'])\n",
        "X_cat['texture_cat'] = pd.qcut(X_cat['mean texture'], 3, labels=['X','Y','Z'])\n",
        "\n",
        "# Drop original columns to keep few numeric and few categorical\n",
        "X_cat = X_cat.drop(['mean radius', 'mean texture'], axis=1)\n",
        "\n",
        "# ---------------------------\n",
        "# Split data\n",
        "# ---------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_cat, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Identify categorical features\n",
        "# ---------------------------\n",
        "cat_features = ['mean_radius_cat', 'texture_cat']\n",
        "\n",
        "# ---------------------------\n",
        "# Train CatBoost model\n",
        "# ---------------------------\n",
        "model = CatBoostClassifier(\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    random_seed=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Hyperparameter tuning\n",
        "# ---------------------------\n",
        "param_dist = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'depth': [4, 6, 8, 10],\n",
        "    'iterations': [100, 200, 500],\n",
        "    'l2_leaf_reg': [1, 3, 5, 7]\n",
        "}\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring='roc_auc',\n",
        "    cv=skf,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train, cat_features=cat_features)\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation\n",
        "# ---------------------------\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_proba))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "hXnefKUaPWJK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}